# ğŸ•µï¸ Stealth Scraper Framework

> **Agentic, self-healing web scraping with advanced stealth capabilities**

A production-ready Python framework combining async scraping, browser fingerprinting evasion, human behavior simulation, and LLM-powered self-healing.

---

## ğŸš€ Features

### Core Capabilities
- âš¡ **Async-first architecture** - High-performance concurrent scraping with `httpx` + `asyncio`
- ğŸ­ **Advanced stealth** - Browser fingerprinting, human behavior simulation, self-healing evasion
- ğŸ¤– **Agentic behavior** - LLM-powered decision making and automatic failure recovery
- ğŸ”„ **Self-healing** - Automatically adapts to site changes and anti-bot measures
- ğŸŒ **Universal** - Works with static HTML and JavaScript-rendered sites

### Stealth System
- **Browser Fingerprinting** - Realistic user agents, screen sizes, fonts, WebGL, canvas
- **Human Behavior** - Natural mouse movements, typing patterns, scroll behavior
- **Cognitive States** - Fatigue simulation, attention modeling, realistic timing
- **Hardware Simulation** - CPU throttling, memory constraints, network jitter

---

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/stealth-scraper.git
cd stealth-scraper

# Install dependencies
pip install -r requirements.txt

# Optional: Install Playwright for browser automation
playwright install chromium
```

---

## ğŸ¯ Quick Start

### Basic Async Scraping with Stealth

```python
import asyncio
from core.scraper import StealthScraper

async def main():
    scraper = StealthScraper()
    
    # Single URL
    result = await scraper.get("https://example.com")
    print(result.text)
    
    # Multiple URLs concurrently
    urls = ["https://site1.com", "https://site2.com", "https://site3.com"]
    results = await scraper.get_many(urls)
    
    await scraper.close()

asyncio.run(main())
```

### Agentic Self-Healing Scraper

```python
from core.agent_scraper import AgentScraper

async def main():
    scraper = AgentScraper(llm_provider="openai")  # or "anthropic"
    
    # Automatically heals when selectors break
    result = await scraper.scrape(
        url="https://example.com",
        extract={
            "title": "h1.title",
            "price": ".product-price",
            "description": "#description"
        }
    )
    
    print(result)

asyncio.run(main())
```

### Browser Automation with Stealth

```python
from core.browser import StealthBrowser

async def main():
    async with StealthBrowser() as browser:
        page = await browser.new_page()
        
        # Automatically applies stealth fingerprinting
        await page.goto("https://example.com")
        
        # Human-like interactions
        await page.human_type("#search", "query text")
        await page.human_click("button[type=submit]")
        
        content = await page.content()
        print(content)

asyncio.run(main())
```

---

## ğŸ—ï¸ Architecture

```
stealth-module/
â”œâ”€â”€ core/                      # Core scraping engine
â”‚   â”œâ”€â”€ scraper.py            # Async HTTP scraper with stealth
â”‚   â”œâ”€â”€ browser.py            # Playwright wrapper with stealth
â”‚   â”œâ”€â”€ agent_scraper.py      # LLM-powered agentic scraper
â”‚   â””â”€â”€ session.py            # Session management & pooling
â”‚
â”œâ”€â”€ stealth-sys/              # Stealth evasion system
â”‚   â”œâ”€â”€ browser-fingerprinting/
â”‚   â”‚   â”œâ”€â”€ user_agents.py
â”‚   â”‚   â”œâ”€â”€ screen_sizes.py
â”‚   â”‚   â”œâ”€â”€ fonts.py
â”‚   â”‚   â””â”€â”€ webgl_canvas.py
â”‚   â”œâ”€â”€ human-behavior/
â”‚   â”‚   â”œâ”€â”€ brain.py          # Cognitive simulation
â”‚   â”‚   â”œâ”€â”€ motion_engine.py  # Mouse movement
â”‚   â”‚   â””â”€â”€ behavioral_models.py
â”‚   â””â”€â”€ self_healing/
â”‚       â””â”€â”€ evasion_tactics.py
â”‚
â”œâ”€â”€ extractors/               # Data extraction utilities
â”‚   â”œâ”€â”€ html_parser.py
â”‚   â”œâ”€â”€ json_extractor.py
â”‚   â””â”€â”€ schema_validator.py
â”‚
â”œâ”€â”€ storage/                  # Data persistence
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ cache.py
â”‚   â””â”€â”€ export.py
â”‚
â”œâ”€â”€ examples/                 # Usage examples
â”‚   â”œâ”€â”€ basic_scraping.py
â”‚   â”œâ”€â”€ agentic_healing.py
â”‚   â”œâ”€â”€ browser_automation.py
â”‚   â””â”€â”€ distributed_scraping.py
â”‚
â””â”€â”€ tests/                    # Test suite
    â”œâ”€â”€ test_scraper.py
    â”œâ”€â”€ test_stealth.py
    â””â”€â”€ test_agent.py
```

---

## ğŸ§  How It Works

### 1. Stealth Layer
Every request automatically includes:
- Realistic browser fingerprints (user agent, screen size, fonts, WebGL)
- Human-like headers and TLS fingerprints
- Natural timing variations and jitter

### 2. Async Engine
- Concurrent requests with `asyncio` and `httpx`
- Connection pooling and session management
- Automatic retries with exponential backoff

### 3. Agentic Self-Healing
When a scraper fails:
1. **Detect** - Identifies the failure (selector not found, CAPTCHA, etc.)
2. **Analyze** - LLM examines the HTML and error context
3. **Adapt** - Generates new selectors or strategies
4. **Retry** - Attempts scraping with the fix
5. **Learn** - Stores successful adaptations

---

## âš™ï¸ Configuration

```python
# config.py
SCRAPER_CONFIG = {
    "max_concurrent": 100,
    "timeout": 30,
    "retry_attempts": 3,
    "stealth_level": "high",  # low, medium, high, paranoid
}

AGENT_CONFIG = {
    "llm_provider": "openai",  # or "anthropic"
    "model": "gpt-4",
    "max_healing_attempts": 3,
    "enable_learning": True,
}

STEALTH_CONFIG = {
    "fingerprint_rotation": True,
    "human_behavior": True,
    "cognitive_simulation": True,
}
```

---

## ğŸ“ Advanced Usage

### Custom Stealth Profiles

```python
from stealth_sys import StealthProfile

profile = StealthProfile(
    browser="chrome",
    os="windows",
    screen_size=(1920, 1080),
    timezone="America/New_York",
)

scraper = StealthScraper(profile=profile)
```

### Distributed Scraping

```python
from core.distributed import DistributedScraper

scraper = DistributedScraper(
    redis_url="redis://localhost:6379",
    workers=10,
)

await scraper.enqueue_urls(urls)
await scraper.start()
```

---

## ğŸ”¬ Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=core --cov=agent --cov=stealth-sys

# Run specific test
pytest tests/test_agent.py -v
```

---

## ğŸ“Š Performance

- **HTTP Scraping**: 10,000+ requests/sec (async)
- **Browser Automation**: 50+ concurrent browser instances
- **Self-Healing**: <2s average recovery time
- **Memory**: ~50MB per scraper instance

---

## ğŸ›¡ï¸ Ethics & Legal

This framework is designed for:
- âœ… Research and education
- âœ… Testing your own applications
- âœ… Legitimate data collection with permission

**NOT for:**
- âŒ Bypassing authentication or paywalls
- âŒ Violating Terms of Service
- âŒ Malicious activities

Always respect `robots.txt` and rate limits.

---

## ğŸ¤ Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md)

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE)

---

## ğŸ™ Acknowledgments

Built with:
- [httpx](https://www.python-httpx.org/) - Async HTTP client
- [Playwright](https://playwright.dev/) - Browser automation
- [OpenAI](https://openai.com/) / [Anthropic](https://anthropic.com/) - LLM APIs

---

**Made with ğŸ”¥ for the scraping community**
